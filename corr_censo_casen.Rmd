---
title: Análisis de correlación en la Casen 2017
author:
- name: VE-CC-CF
  affiliation: DataIntelligence
subtitle: | 

date: "Viernes 4-06-2021"

abstract: |
  Debemos descubrir correlaciones dentro de las encuesta Casen. La dificultad es que, en general, la información ofrecida es de tipo categórico, por lo que se deben aplicar test no paramétricos. En éste informe y como primer desarrollo los aplicaremos en el estudio de la correlacion ingreso (variable continua) y pertenencia al DAU autonomo, una variable categórica.
  Queremos comparar si hay diferencias en el valor de una variable dependiente entre tres o más grupos.
    
header-includes:
   - \usepackage[]{babel}

output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
#library(ggpubr)
library(markdown)
library(shiny)
library(shinythemes)
library(tidyverse)
library(magrittr)
library(lubridate)
#library(plotly)
library(kableExtra)
library(knitr)
library("readxl")
library(writexl)
#library(RPostgreSQL)
#library(devtools)
library(remotes)
library(DBI)
library(tidyverse)
library(kableExtra)
#library(reldist)
library("readxl")
library("writexl")
library(kableExtra)

```

# 1 Introducción


Determinar cuáles variables de la CASEN pueden ser utilizadas para realizar predicciones de a nivel de Zona Censal  utilizando los datos del Censo 2017


Comparemos el ingreso que reciben las personas bajo 10 condiciones distintas. ¿Existen diferencias significativas dependiendo de ellas?

## 2 Analisis exploratorio y tratamiento de los datos

```{r}
ab <- readRDS(file = "casen_2017_c.rds")
```
















```{r}
# ab <- ab[c(1:1000),]
```





ytotcor: ingreso total corregido
dau: Decil autónomo nacional 1 I - 10 X

```{r}
ingresos <- ab$ytotcor
dau <- ab$dau
nuevo_df <- cbind(ingresos, dau)
head(nuevo_df,10)
```

reemplazamos los NA por el promedio

```{r}
# 
library(zoo)
nuevo_df_sin_NA <- na.aggregate(nuevo_df)
head(nuevo_df_sin_NA,10)
```

convertimos a dataframe

```{r}
nuevo_df_sin_NA <- as.data.frame(nuevo_df_sin_NA)
head(nuevo_df_sin_NA,10)
```

calculamos los promedios por grupo

```{r}
aggregate(ingresos ~ dau, data = nuevo_df_sin_NA, FUN = mean)
```

calculamos la Desviacion estandar

```{r}
aggregate(ingresos ~ dau, data = nuevo_df_sin_NA, FUN = sd)
```


Excluimos los outliers

```{r}
# 
Q <- quantile(nuevo_df_sin_NA$ingresos, probs=c(.25, .75), na.rm = T)

iqr <- IQR(nuevo_df_sin_NA$ingresos)

eliminated <- subset(nuevo_df_sin_NA, nuevo_df_sin_NA$ingresos > (Q[1] - 1.5*iqr) & nuevo_df_sin_NA$ingresos < (Q[2]+1.5*iqr))
eliminated <- data.frame(lapply(eliminated, as.character), stringsAsFactors=FALSE)

# despleguemos los primeros 100 registros en pantalla del subset creado:
# eliminated_100 <- eliminated[c(1:100),]
```

```{r}

#eliminated <- as.double(eliminated)
head(eliminated,10)
```

convirtamos a matriz:

```{r}
a = as.double(eliminated$ingresos) 
b = as.double(eliminated$dau) 
a_b <- cbind(a,b)
head(a_b,10)
```




```{r}
boxplot(a ~ b, data=a_b, col="green", cex.axis=0.7,las = 2, ylab="var1", xlab="Treatments", cex.lab=0.75)
```


## ANOVA

Hacemos un modelo ANOVA para nuestra Variable 1 (se haría lo mismo para la Variable 2):

```{r}
nuestraanova <- lm(ingresos ~ dau, data=eliminated)
nuestromodelo <- anova(nuestraanova)
nuestromodelo
```

¿Son nuestros datos normales y homogéneos?

El test ANOVA requiere normalidad de los residuos y homogeneidad en las varianzas.

Representamos visualmente los residuos:

```{r}
par(mfrow=c(2,2))
plot(nuestraanova)
```

```{r}
#shapiro.test(residuals(nuestraanova))
```

```{r}
 #fligner.test(ingresos ~ deciles, data=eliminated)
```


```{r}
head(eliminated,10)
```

```{r}
eliminated$ingresos <- as.double(eliminated$ingresos)
eliminated$dau <-  as.double(eliminated$dau)
head(eliminated,10)
```



## Hacemos un test Kruskal-Wallis:



```{r}
 kruskal.test(ingresos~ dau, data = eliminated)
```

```{r}
pairwise.wilcox.test(eliminated$ingresos,eliminated$dau,
                     p.adjust.method = c("bonferroni"),paired=F,exact=F)
```




```{r}
pairwise.wilcox.test(x = eliminated$
ingresos, g = eliminated$dau, p.adjust.method = "holm" )
```



# II Correlaciones entre dos variables categoricas:

```{r}
ab <- readRDS(file = "casen_2017_c.rds")
```

```{r}
num_personas <- ab$numper
escolari <- ab$esc
num_escolari <- cbind(num_personas,escolari)
head(num_escolari,10)
```

```{r}
xtabs(~ num_personas + escolari, data=num_escolari)
```



```{r}
num_escolari_sin_NA <- na.aggregate(num_escolari)
```

```{r}
chisq.test(num_escolari_sin_NA)
```




Referencias:

ANOVA y Kruskal-Wallis\
https://rpubs.com/aafernandez1976/anovakw



I Test Kruskal-Wallis\
https://rpubs.com/Joaquin_AR/219504


Análisis de variables categóricas con R\
https://biocosas.github.io/R/060_analisis_datos_categoricos.html
















































<!-- # I: Prueba de correlación entre dos variables en R  -->

<!-- La prueba de correlación se utiliza para evaluar la asociación entre dos o más variables.  -->

<!-- ```{r} -->
<!-- library("ggpubr") -->
<!-- ``` -->

<!-- Métodos de análisis de correlación -->

<!--    Correlación de Pearson (r), que mide una dependencia lineal entre dos variables (x e y). También se conoce como prueba de correlación paramétrica porque depende de la distribución de los datos. Solo se puede usar cuando x e y son de distribución normal. La gráfica de $y = f(x)$ se denomina curva de regresión lineal. -->

<!--    La tau de Kendall y el rho de Spearman, que son coeficientes de correlación basados en rangos (no paramétricos) -->

<!-- El método más utilizado es el método de correlación de Pearson.  -->

<!-- ```{r} -->
<!-- cor(ab$e6b, ab$ytot,  method = "pearson", use = "complete.obs") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- abe6b_num <- as.numeric(as.character(ab$e6b)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- cor(abe6b_num, ab$ytot,  method = "pearson", use = "complete.obs") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ggscatter( ab, abe6b_num, ab$ytot,  -->
<!--           add = "reg.line", conf.int = TRUE,  -->
<!--           cor.coef = TRUE, cor.method = "pearson", -->
<!--           xlab = "Miles/(US) gallon", ylab = "Weight (1000 lbs)") -->
<!-- ``` -->


<!-- Decil autónomo nacional: DAU -->
<!-- Escolaridad: esc -->

<!-- ```{r} -->
<!-- ab$y101 <- as.character(ab$ytotcor) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ab$y102 <- as.character(ab$esc) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- ggscatter( ab,  ab$y101, ab$y102,  -->
<!--           add = "reg.line", conf.int = TRUE,  -->
<!--           cor.coef = TRUE, cor.method = "pearson", -->
<!--           xlab = "Miles/(US) gallon", ylab = "Weight (1000 lbs)") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- cor(ab, method = "spearman") -->
<!-- ``` -->
<!-- ```{r} -->
<!-- corr -->
<!-- ``` -->




<!-- Calcular la correlación en R -->

<!-- Funciones R -->

<!-- El coeficiente de correlación se puede calcular usando las funciones cor () o cor.test ():  -->

<!-- i. cor() calcula el coeficiente de correlación -->

<!-- ii. cor.test () prueba de asociación/correlación entre muestras apareadas. Devuelve tanto el coeficiente de correlación como el nivel de significancia (o valor p) de la correlación.  -->


<!-- ```{r} -->

<!-- ``` -->





<!-- # II: Matriz de correlación -->

<!-- Previamente, describimos cómo realizar la prueba de correlación entre dos variables. Ahora necesitamos calcular una matriz de correlación, que se utiliza para investigar la dependencia entre varias variables al mismo tiempo. El resultado es una tabla que contiene los coeficientes de correlación entre cada variable y las demás. -->


<!-- Existen diferentes métodos para el análisis de correlación: prueba de correlación paramétrica de Pearson, análisis de correlación basado en rangos de Spearman y Kendall. Estos métodos se analizan en las siguientes secciones. -->

<!-- Calcular la matriz de correlación en R -->

<!-- Funciones R -->

<!-- La función R cor() se puede utilizar para calcular una matriz de correlación. Un formato simplificado de la función es: -->

<!--       cor (x, método = c ("pearson", "kendall", "spearman"))  -->


<!-- ```{r} -->
<!-- ab -->
<!-- ``` -->



<!-- ```{r} -->
<!-- cor (ab, method = c ("pearson"))  -->
<!-- ``` -->

<!-- Como regla general, cuando el nivel de medición de la variable dependiente es nominal (categórica) u ordinal, se debe seleccionar una prueba no paramétrica. Cuando la variable dependiente se mide en una escala continua, normalmente se debe seleccionar una prueba paramétrica.  -->






<!-- # III: Visualice la matriz de correlación usando correlograma  -->
<!-- # IV: Elegante tabla de correlación con el paquete xtable R  -->
<!-- # V: Matriz de correlación: una función R para hacer todo lo que necesita  -->























<!-- ```{r} -->
<!-- # Eliminamos del dataframe los registros que no posean valores en la columna del ingreso total: -->
<!-- ab <- ab[!is.na(ab$ytotcor),] -->

<!-- ``` -->

<!-- ```{r} -->
<!-- # Excluimos los outliers -->
<!-- Q <- quantile(ab$ytotcor, probs=c(.25, .75), na.rm = FALSE) -->
<!-- iqr <- IQR(ab$ytotcor) -->
<!-- eliminated <- subset(ab, ab$ytotcor > (Q[1] - 1.5*iqr) & ab$ytotcor < (Q[2]+1.5*iqr)) -->
<!-- eliminated <- data.frame(lapply(eliminated, as.character), stringsAsFactors=FALSE) -->

<!-- # despleguemos los primeros 100 registros en pantalla del subset creado: -->
<!-- eliminated_100 <- eliminated[c(1:100),] -->
<!-- # eliminated_100 %>%  kbl() %>% -->
<!-- # kable_material(c("striped", "hover"), font_size = 12)%>% -->
<!-- #    scroll_box(width = "100%", height = "500px") -->
<!-- ``` -->

<!-- ```{r, message=FALSE, warning=FALSE, results='hide'} -->
<!-- b <-  as.numeric(eliminated$ytotcor) -->
<!-- a <- eliminated$comuna -->


<!-- # Calculamos los promedios de ingreso grupales, la desviación estandar del rango y el coeficiente de Gini: -->
<!-- promedios_grupales <-aggregate(b, by=list(a), FUN = mean , na.rm=TRUE ) -->
<!-- promedios_grupales_sd <-aggregate(b, by=list(a), FUN = sd , na.rm=TRUE ) -->


<!-- # Asignamos nuevas columnas a la tabla base con medias y sd: -->
<!-- promedios_grupales$sd <- promedios_grupales_sd$x -->


<!-- # Eliminamos los valores que no tengan desviacion standard, pues implican registros unicos. -->
<!-- promedios_grupales <-promedios_grupales[!(is.na(promedios_grupales$sd)),] -->

<!-- # promedios_grupales %>%  kbl() %>% kable_material(c("striped", "hover"), font_size = 12)%>% -->
<!-- #    scroll_box(width = "100%", height = "500px") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- head(promedios_grupales,5) -->
<!-- ``` -->





<!-- ```{r} -->
<!-- #Asignamos nombres con sentido a las cabeceras: -->
<!-- names(promedios_grupales)[1] <- "comuna" -->
<!-- names(promedios_grupales)[2] <- "promedio" -->
<!-- promedios_grupales$año <- "2017" -->
<!-- ``` -->






<!-- ```{r} -->
<!-- codigos_comunales <- readRDS(file = "codigos_comunales_2011-2017.rds") -->
<!-- names(codigos_comunales)[1] <- "código" -->
<!-- names(codigos_comunales)[2] <- "comuna" -->
<!-- ``` -->

<!-- ```{r} -->
<!-- df_2017 = merge( x = promedios_grupales, y = codigos_comunales, by = "comuna", all.x = TRUE) -->

<!-- ``` -->



<!-- ```{r} -->
<!-- head(df_2017,5) -->
<!-- ``` -->





<!-- ```{r} -->
<!-- censo_2017 <- readRDS("censo_personas_con_clave_17.rds") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- head(censo_2017,5) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- promedios_de_esco <-aggregate(censo_2017$P15, by=list(censo_2017$COMUNA_15R), FUN = mean , na.rm=TRUE ) -->


<!-- names(promedios_de_esco)[1] <- "código" -->
<!-- ``` -->

<!-- ```{r} -->
<!-- head(promedios_de_esco,5) -->
<!-- ``` -->



<!-- ```{r} -->
<!-- # recogemos el campo Comuna: -->
<!-- codigos <- promedios_de_esco$código -->
<!-- # construimos una secuencia llamada rango del 1 al total de filas del dataset: -->
<!-- rango <- seq(1:nrow(promedios_de_esco)) -->
<!-- # Creamos un string que agrega un cero a todos los registros: -->
<!-- cadena<- paste("0",codigos[rango], sep = "") -->

<!-- # El string cadena tiene o 5 o 6 digitos, los cuales siempre deben ser siempre 5 agregandole un cero al inicio de los que tienen 4. -->
<!-- # Para ello extraemos un substring de la cadena sobre todas las filas (rangos) comenzando desde el primero o el segundo y llegando siempre al 6. -->

<!-- cadena <- substr(cadena,(nchar(cadena)[rango])-(4),6) -->
<!-- codigos <- as.data.frame(codigos) -->
<!-- cadena <- as.data.frame(cadena) -->
<!-- comuna_corr <- cbind(codigos,cadena) -->
<!-- ``` -->



<!-- ```{r} -->


<!-- names(comuna_corr)[1] <- "código" -->
<!-- head(comuna_corr,5) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- arbolito <- merge(x=promedios_de_esco , y=comuna_corr, by = "código") -->
<!-- ``` -->


<!-- ```{r} -->
<!-- head(arbolito,5) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- arbolito2 <- merge(x=arbolito , y=df_2017, by = "código") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- head(arbolito2,5) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- cor(arbolito2$x, arbolito2$promedio, method = c("pearson")) -->
<!-- ``` -->



<!-- Referencias: -->


<!-- Correlation Analyses in R \ -->
<!-- http://www.sthda.com/english/wiki/correlation-analyses-in-r -->



